## Maskinlæring og dataanalyse

* [Search-based structured prediction](http://hunch.net/~jl/projects/reductions/searn/searn.pdf)
* [Artificial Intelligence, poker and regret](https://medium.com/@RemiStudios/artificial-intelligence-poker-and-regret-part-1-36c78d955720)
* [Entropy Maximization and intelligent behaviour](http://paulispace.com/intelligence/2017/07/06/maxent.html)
* [AI Cheatsheets](https://github.com/kailashahirwar/cheatsheets-ai)
* [Liste over selskaper som driver med ML](https://medium.com/@shivon/the-current-state-of-machine-intelligence-3-0-e4d305da032e#.98syd18r3)

### Case studies og løsningsforslag

* [Using machine learning to predict gender](https://www.crowdflower.com/using-machine-learning-to-predict-gender/)
* [My solution for the Galaxy Zoo challenge](http://benanne.github.io/2014/04/05/galaxy-zoo.html)
* [Insights into a corpus of 2.5 million news headlines](https://freedom-to-tinker.com/2016/09/14/all-the-news-thats-fit-to-change-insights-into-a-corpus-of-2-5-million-news-headlines/)

### NLP

* [Clickbait classifier](https://github.com/peterldowns/clickbait-classifier)
* [News Sniffer](https://github.com/johnl/news-sniffer)

### Bilde- og lydanalyse

* [WaveNet: A Generative Model for Raw Audio](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)
* [WaveFunctionCollapse](https://github.com/mxgmn/WaveFunctionCollapse)
    - [Min Java-port av WFC](https://github.com/trygvebw/JavaWaveFunctionCollapse)
* Bildegjenkjenning og delimitering fra Facebook:
    - https://github.com/facebookresearch/deepmask
    - https://github.com/facebookresearch/multipathnet

### Optimering

* [How does the Adam method of stochastic gradient descent work?](https://stats.stackexchange.com/questions/220494/how-does-the-adam-method-of-stochastic-gradient-descent-work)
* [An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf)
* [Bayesian global optimization with Gaussian Processes](https://github.com/fmfn/BayesianOptimization)
* [Bayesiansk optimering ved hjelp av gaussiske prosesser](https://www.reddit.com/r/MachineLearning/comments/27lrxo/bayesian_optimization_with_gaussian_processes/)
* [Spearmint, a package to perform Bayesian optimization](https://github.com/JasperSnoek/spearmint)
* [Particle swarm optimization](https://en.wikipedia.org/wiki/Particle_swarm_optimization)
* [Bayesian optimization på Wikipedia](https://en.wikipedia.org/wiki/Bayesian_optimization)

### Eksotiske modeller og ensembling

* [ClassifierChain i scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.ClassifierChain.html#sklearn.multioutput.ClassifierChain)
* [Regression Phalanxes](https://arxiv.org/pdf/1707.00727.pdf)
* [Online Passive-Aggressive Algorithms](http://www.jmlr.org/papers/volume7/crammer06a/crammer06a.pdf)
* [Passive Aggressive learning](http://scikit-learn.org/stable/modules/linear_model.html#passive-aggressive-algorithms)

### Tolking av modeller

Se også under hver enkelt modell.

* [Skater, model agnostic interpretation library](https://github.com/datascienceinc/Skater)
* [The Mythos of Model Interpretability](https://arxiv.org/abs/1606.03490)
* [Help with interpretation of trained networks](https://www.reddit.com/r/MachineLearning/comments/5mzmig/d_help_with_interpretation_of_trained_networks/)

### Lenker fra O'Reilly AI New York 2017

* [BayesDB](http://probcomp.csail.mit.edu/bayesdb/)
* [The Automatic Statistician](https://www.automaticstatistician.com/index/)
* [Stan](http://mc-stan.org/)
* https://conferences.oreilly.com/artificial-intelligence/ai-ny/public/schedule/detail/59164
* [MIT Probabilistic Computing Project](http://web.mit.edu/vkm/www/)
* [Q learning](https://en.wikipedia.org/wiki/Q-learning)

### Semi-supervised learning

* [An introduction to biclustering](http://www.kemaleren.com/an-introduction-to-biclustering.html)
* [Co-training](https://en.wikipedia.org/wiki/Co-training)
* [Sklearn Semi-Supervised](http://scikit-learn.org/stable/modules/label_propagation.html)
* [Semi-supervised Logistic Regression](http://ama.liglab.fr/~amini/Publis/SemiSupLogReg_ecai02.pdf)
* [Semi-supervised learning frameworks for Python](https://github.com/tmadl/semisup-learn)
* [Semi-supervised learning tutorial](http://pages.cs.wisc.edu/~jerryzhu/pub/sslicml07.pdf)

### Unsupervised learning, clustering

* [Spectral Clustering: Intuition and Implementation](https://www.dropbox.com/s/cpmhu4xaig84dj7/spectral.pdf)
* [Learning in latent variable models](https://ermongroup.github.io/cs228-notes/learning/latent/)
* [Mixture model](https://en.wikipedia.org/wiki/Mixture_model)
* [Comparing Latent Class Factor Analysis with the Traditional Approach in Data Mining](http://web.archive.org/web/20130502181643/http://www.statisticalinnovations.com/articles/bozdogan.pdf)
* [Latent Class Analysis vs Cluster Analysis](https://stats.stackexchange.com/questions/122213/latent-class-analysis-vs-cluster-analysis-differences-in-inferences)
* [Spectral clustering](https://en.wikipedia.org/wiki/Spectral_clustering)
* [Association rule learning](https://en.wikipedia.org/wiki/Association_rule_learning)
    - Oppdagelse av sammenhenger mellom variabler
* [Robust Continuous Clustering](https://www.reddit.com/r/MachineLearning/comments/6x8gh2/r_pretty_exciting_new_method_of_clustering_robust/)
* [How to understand the drawbacks of K-means](https://stats.stackexchange.com/questions/133656/how-to-understand-the-drawbacks-of-k-means)

### Dimensjonalitetsreduksjon

* [t-SNE, eksempler og FAQ](https://lvdmaaten.github.io/tsne/)
* [How to use T-SNE effectively](https://distill.pub/2016/misread-tsne/)

### Nyttige ting i biblioteker

* [Model persistence i sklearn](http://scikit-learn.org/stable/modules/model_persistence.html)

### Validering og modelvurdering

* [Is hold-out validation a better approximation of "getting new data" than k-fold CV?](http://stats.stackexchange.com/questions/108345/is-hold-out-validation-a-better-approximation-of-getting-new-data-than-k-fold?noredirect=1&lq=1)
* [Is cross-validation a proper substitute for validation set?](http://stats.stackexchange.com/questions/18856/is-cross-validation-a-proper-substitute-for-validation-set?rq=1)
* [Why do researchers use 10-fold cross validation instead of testing on a validation set?](http://stats.stackexchange.com/questions/49692/why-do-researchers-use-10-fold-cross-validation-instead-of-testing-on-a-validati?rq=1)
* [Why is it wrong to use non-nested cross-validation?](http://stats.stackexchange.com/questions/179029/why-is-it-wrong-to-use-non-nested-cross-validation?rq=1)
* [How can I help ensure testing data does not leak into training data?](http://stats.stackexchange.com/questions/20010/how-can-i-help-ensure-testing-data-does-not-leak-into-training-data)

### Lineær regresjon og relaterte modeller

* [When should I use LASSO vs ridge?](https://stats.stackexchange.com/questions/866/when-should-i-use-lasso-vs-ridge)
* [Need advice on finding good regularization parameter for ridge regression](https://www.reddit.com/r/MachineLearning/comments/1gbrgs/need_advice_on_finding_good_regularization/)
* [Kernel ridge regression, Gaussian processes, and ensemble methods](https://people.eecs.berkeley.edu/~bartlett/courses/281b-sp08/10.pdf)
* [Is regression with L1 regularization the same as Lasso, and with L2 regularization the same as ridge regression?](https://stats.stackexchange.com/questions/200416/is-regression-with-l1-regularization-the-same-as-lasso-and-with-l2-regularizati?noredirect=1&lq=1)

### Trær, random forests, gradient boosting og regelmodeller

* [Predictive Learning via Rule Ensembles (artikkel om Rulefit)](http://statweb.stanford.edu/~jhf/ftp/RuleFit.pdf)
* [Memory-Efficient Global Refinement of Decision-Tree Ensembles](https://arxiv.org/abs/1702.08481)
* [Difference between rulefit and random forest](https://stats.stackexchange.com/questions/208930/difference-between-rulefit-and-random-forest)
* [RuleFit i Python](https://github.com/christophM/rulefit)
* [Highly interpretable, sklearn-compatible classifier based on decision rules](https://github.com/tmadl/sklearn-expertsys)
* [Bayesian Regression Tree Ensembles that Adapt to Smoothness and Sparsity](https://arxiv.org/abs/1707.09461)
* [Bayesian and Empirical Random Forests](https://arxiv.org/pdf/1502.02312.pdf)
* [BART: Bayesian additive regression trees](https://arxiv.org/abs/0806.3286)
* [Big Data Classification Using Augmented Decision Trees](https://arxiv.org/abs/1710.09567)
* [Rotation forest: A new classifier ensemble method](https://www.ncbi.nlm.nih.gov/pubmed/16986543)
* [The often-overlooked random forest kernel](https://rmarcus.info/blog/2017/10/04/rfk.html)
* [Forest confidence interval](https://github.com/scikit-learn-contrib/forest-confidence-interval)
* [Forward thinking: Building Deep Random Forests](https://arxiv.org/abs/1705.07366)
* [Are categorical variables getting lost in your random forest?](https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/)
* [Similarity Forests](http://www.kdd.org/kdd2017/papers/view/similarity-forests)
* [Canonical Correlation Forests](https://arxiv.org/abs/1507.05444)
    - [ccf](https://github.com/jandob/ccf)
* [How are random forests not susceptible to outliers?](https://stats.stackexchange.com/questions/187200/how-are-random-forests-not-sensitive-to-outliers)
* [Observations on Bagging](http://www.stat.washington.edu/wxs/Learning-papers/paper-bag.pdf)
* [Why does bagging use bootstrap samples?](https://stats.stackexchange.com/questions/109853/why-does-bagging-use-bootstrap-samples?rq=1)
* [PyEearth, MARS for Python](https://github.com/scikit-learn-contrib/py-earth)
* [Is MARS better than neural networks?](https://www.casact.org/pubs/forum/03spforum/03spf269.pdf)
* [CHAID vs CRT (or CART)](https://stats.stackexchange.com/questions/61230/chaid-vs-crt-or-cart/61245#61245)
* [CHAID for Python](https://github.com/Rambatino/CHAID)
* [CatBoost, an open source gradient boosting library](https://catboost.yandex/)
* [Decision Stream: Cultivating Deep Decision Trees](https://arxiv.org/pdf/1704.07657.pdf)
* [Of decision trees and genetic algorithms](http://numbersandcode.com/of-decision-trees-and-genetic-algorithms)
* [Bagging with oversampling for rare event predictive models](https://stats.stackexchange.com/questions/15036/bagging-with-oversampling-for-rare-event-predictive-models/15082)
* [What are some shortcomings of random forests?](https://www.quora.com/Ensemble-Learning-What-are-some-shortcomings-of-random-forests)
* [FEST - Fast Ensembles of Sparse Trees](http://lowrank.net/nikos/fest/)
* [Accelerating the XGBoost algorithm using GPU computing](https://peerj.com/preprints/2911/)

#### Visualisering og tolking av tremodeller

* [Why and how use random forest variable importance measures](http://www.statistik.uni-dortmund.de/useR-2008/slides/Strobl+Zeileis.pdf)
* [Forest floor visualizations of Random Forest](http://forestfloor.dk/)
    - [Implementasjon i R](https://github.com/sorhawell/forestFloor)
* [Making Tree Ensembles Interpretable](https://arxiv.org/pdf/1606.05390.pdf)
* [Random forest interpretation with scikit-learn](http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/)
* [Interpreting random forests](http://blog.datadive.net/interpreting-random-forests/)
* [Feature Correlation and Feature Importance Bias with Random Forests](http://rnowling.github.io/machine/learning/2015/08/11/random-forest-correlation-bias.html)
* [What are some proven methods to get probability values from Random Forests?](https://www.quora.com/What-are-some-proved-methods-to-get-probability-values-from-Random-Forests)
* [Obtaining knowledge from a random forest](https://stats.stackexchange.com/questions/21152/obtaining-knowledge-from-a-random-forest)

### Gaussiske prosesser

* [Gaussian processes for machine learning (bok)](http://www.gaussianprocess.org/gpml/chapters/RW.pdf)
* [Deep Gaussian processes](http://inverseprobability.com/2015/02/28/questions-on-deep-gaussian-processes)
* [Deep limits](https://raw.githubusercontent.com/duvenaud/phd-thesis/master/deeplimits.pdf)
* [Gaussian Processes for NLP](http://people.eng.unimelb.edu.au/tcohn/papers/gptut_alta14.pdf)
* [The Kernel Cookbook: Advice on Covariance functions](http://www.cs.toronto.edu/~duvenaud/cookbook/index.html)
* [Variational Fourier Features for Gaussian Processes](https://arxiv.org/pdf/1611.06740v1.pdf)
* [New Boosting Methods of Gaussian Processes for Regression](http://bigeye.au.tsinghua.edu.cn/english/paper/ijcnn05_syq_zcs.pdf)
* [What is an intuitive explanation of Gaussian process models?](https://www.quora.com/What-is-an-intuitive-explanation-of-Gaussian-Process-Models)
* [Understanding Gaussian Process Regression using the equivalent kernel](http://homepages.inf.ed.ac.uk/ckiw/postscript/sheff3.pdf)
* [Gaussian processes for regression: A quick introduction](https://www.robots.ox.ac.uk/~mebden/reports/GPtutorial.pdf)
* [Fast methods for training Gaussian processes on large datasets](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4892455/)
* [Kommentar om state of the art i approksimative gaussiske prosesser](https://www.reddit.com/r/MachineLearning/comments/3x562c/choosing_points_for_a_gaussian_process_subset_of/cy1wci8/)
* [Is Gaussian Processes useful in context of large-scale machine learning?](https://www.reddit.com/r/MachineLearning/comments/4rhwr3/is_gaussian_process_useful_in_context_largescale/)
* [Gaussian processes for Big Data](https://arxiv.org/pdf/1309.6835v1.pdf)
* [GPy](https://github.com/SheffieldML/GPy)
    - [Sparse GP Regression](http://nbviewer.jupyter.org/github/SheffieldML/notebook/blob/master/GPy/sparse_gp_regression.ipynb)
    - [Stochastic Variational Inference with Gaussian Processes](http://nbviewer.jupyter.org/github/SheffieldML/notebook/blob/master/GPy/SVI.ipynb)
    - [Sparse GPs and likelihood approximation](http://gpss.cc/gpss13/assets/lab2.pdf)
    - [Gaussian process regression tutorial](http://nbviewer.jupyter.org/github/SheffieldML/notebook/blob/master/GPy/basic_gp.ipynb)
    - [Some questions about GP](https://github.com/SheffieldML/GPy/issues/124)
* [GPflow, gaussiske prosesser på Tensorflow](https://github.com/GPflow/GPflow)

### Kernel density estimation

* [Multivariate kernel density estimation](https://en.wikipedia.org/wiki/Multivariate_kernel_density_estimation)
* [Kernel mixture networks](https://janvdvegt.github.io/2017/06/07/Kernel-Mixture-Networks.html)

### Generaliserte additive metoder (GAM)

* [GAM: The predictive modeling silver bullet](http://multithreaded.stitchfix.com/blog/2015/07/30/gam/)
* [Generalized linear models and Generalized additive models](http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch13.pdf)

### SVM

* [Support Vector Machine for Complex Outputs](https://www.cs.cornell.edu/people/tj/svm_light/svm_struct.html)

### Nevrale nettverk

* [Do numerical inaccuracies play any role in training neural networks?](https://datascience.stackexchange.com/questions/20271/do-numerical-inaccuracies-play-any-role-in-training-neural-networks)
* [Visualizing representations](http://colah.github.io/posts/2015-01-Visualizing-Representations/)
* [LSTM by example using Tensorflow](https://medium.com/towards-data-science/lstm-by-example-using-tensorflow-feb0c1968537)
* [Slides om Bayesian Deep Learning](http://bayesiandeeplearning.org/slides/nips16bayesdeep.pdf)
* [Bayesian Deep Learning](http://twiecki.github.io/blog/2016/06/01/bayesian-deep-learning/)
* [Random-Walk Bayesian Deep Networks: Dealing With Non-Stationary Data](http://twiecki.github.io/blog/2017/03/14/random-walk-deep-net/)
* [37 reasons why your neural network is not working](https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607)
* [Introduction to debugging neural networks](http://russellsstewart.com/notes/0.html)
* [Everything that works works because it's Bayesian](http://www.inference.vc/everything-that-works-works-because-its-bayesian-2/)
* [Visuell (animert) sammenligning av ulike versjon av SGD](https://stackoverflow.com/a/43930924/1004065)
* [How does mini-batch size affect the performance of SGD?](https://www.quora.com/Intuitively-how-does-mini-batch-size-affect-the-performance-of-stochastic-gradient-descent)
* [Bayesian deep learning for safe AI](http://alexgkendall.com/computer_vision/bayesian_deep_learning_for_safe_ai/)
* [Bayesian Deep Learning with Edward](https://www.youtube.com/watch?v=I09QVNrUS3Q)
* [What are the advantages of using a Bayesian neural network?](http://stats.stackexchange.com/questions/141879/what-are-the-advantages-of-using-a-bayesian-neural-network)
* [Bayesian Neural Network](http://edwardlib.org/tutorials/bayesian-neural-network)
* [Bayesian Learning for Neural Networks](https://link.springer.com/book/10.1007%2F978-1-4612-0745-0)

### Nearest neighbours o.l.

* [Annoy](https://github.com/spotify/annoy)

### Tidsrekker

* [Tick, statistical learning for time dependent systems library](https://x-datainitiative.github.io/tick/)
* [seasonal – Robustly estimate trend and periodicity in a timeseries](https://github.com/welch/seasonal)
* [Strategies for Multi-Step Time Series Forecasting](https://machinelearningmastery.com/multi-step-time-series-forecasting/)
* [A review and comparison of strategies for multi-step ahead time series
forecasting based on the NN5 forecasting competition](https://arxiv.org/pdf/1108.3259.pdf)
* [Features to encode the time-evolving nature of a time series](https://stats.stackexchange.com/questions/265541/features-to-encode-the-time-evolving-nature-of-a-time-series?noredirect=1&lq=1)
* [How to use embedding with 3D tensor in Keras](https://datascience.stackexchange.com/questions/22177/how-to-use-embedding-with-3d-tensor-in-keras)
* [VARMAX](http://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.varmax.VARMAX.html)
* [Prophet, et Python-bibiliotek for tidsrekkeforecasting](https://github.com/facebookincubator/prophet)
* [Variable Sequence Lengths in TensorFlow](https://danijar.com/variable-sequence-lengths-in-tensorflow/)
* [Applying Deep Learning to Time Series Forecasting with TensorFlow](https://mapr.com/blog/deep-learning-tensorflow/)

### Preprosessering, feature engineering, o.l.

* [VarianceThreshold i sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html)
* [What is the purpose of row normalization?](https://stats.stackexchange.com/questions/175463/what-is-the-purpose-of-row-normalization)
* [Discover feature engineering, how to engineer features and how to get good at it](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)
* [Feature engineering versus feature extraction: game on!](http://appliedpredictivemodeling.com/blog/2015/7/28/feature-engineering-versus-feature-extraction)
* [Hashing trick](https://en.wikipedia.org/wiki/Feature_hashing)
* [PCA Explained Visually](http://setosa.io/ev/principal-component-analysis/)
* [Stopping rules in PCA: A comparison of heuristical and statistical approaches](http://onlinelibrary.wiley.com/doi/10.2307/1939574/abstract)
* [What is the difference between ZCA whitening and PCA whitening?](https://stats.stackexchange.com/questions/117427/what-is-the-difference-between-zca-whitening-and-pca-whitening)
* [Whitening](http://ufldl.stanford.edu/wiki/index.php/Whitening)
* [Whitening transformation](https://en.wikipedia.org/wiki/Whitening_transformation)
* [Should you apply PCA to your data?](http://blog.explainmydata.com/2012/07/should-you-apply-pca-to-your-data.html)
* [Bayesian PCA](https://www.microsoft.com/en-us/research/publication/bayesian-pca/)
* [Determining the number of components in PCA](http://onlinelibrary.wiley.com/doi/10.2307/1939574/abstract)

### Anomalianalyse

* [PyNomaly, et bibliotek for å gjøre anomalianalyse i Python](https://github.com/vc1492a/PyNomaly)
* [IsolationForest i sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html)

## Probabilistisk programmering, bayesianske nettverk og MRF-er

* [Probabilistic programming from scratch](http://blog.fastforwardlabs.com/2017/07/06/probabilistic-programming-from-scratch.html)
* [Dynamic Bayesian network](https://en.wikipedia.org/wiki/Dynamic_Bayesian_network)
* [Is there any domain where Bayesian Networks outperform neural networks?](https://www.reddit.com/r/MachineLearning/comments/41dij6/is_there_any_domain_where_bayesian_networks/)
* [Influence diagram](https://en.wikipedia.org/wiki/Influence_diagram)
* [Probabilistic Soft Logic](http://psl.linqs.org/)
* [Bayesian Belief Network Python Tutorial](http://bayesian-networks.blogspot.no/2014/08/bayesian-belief-network-python-tutorial.html)

### Biblioteker for probabilistisk programmering

* [Markov Logic Networks in Python](http://www.pracmln.org/)
* [bayesian-belief-networks](https://github.com/eBay/bayesian-belief-networks)
* [Hello, world! Stan, PyMC3, and Edward](http://andrewgelman.com/2017/05/31/compare-stan-pymc3-edward-hello-world/)
* [BayesPy](http://www.bayespy.org/)
* [PEBL](https://github.com/abhik/pebl)
* [libpgm](http://pythonhosted.org/libpgm/)
* [pomegranate](https://github.com/jmschrei/pomegranate)
* [emcee](http://dan.iel.fm/emcee/)
* [PyMC, Bayesian statistical modeling and MCMC Machine Learning in Python](https://github.com/pymc-devs/pymc3)
* [Edward, a library for probabilistic modeling, inference and criticism](http://edwardlib.org/)
* [PyStan](https://pystan.readthedocs.io/en/latest/)

## Datasett

* [Awesome Public Datasets](https://github.com/caesar0301/awesome-public-datasets)
* [List of datasets for machine learning research](https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research)
* [Free datasets](https://webhose.io/datasets)
* [Yeast data set](https://archive.ics.uci.edu/ml/datasets/Yeast)
* [Hill-valley data set](https://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/)

## Visualisering av data

* [A History of the Display of Information](https://i.redd.it/mpzqq901oi9y.jpg)
* [William Playfair](https://en.wikipedia.org/wiki/William_Playfair)
* [The Data Visualisation Catalogue](https://datavizcatalogue.com/)
* [Altair](https://altair-viz.github.io/)
* [Vega](https://vega.github.io/)
* [Toyplot, plotting i Python](http://toyplot.readthedocs.io/en/stable/tutorial.html)
* [Gallery of Data Visualization](http://www.datavis.ca/gallery/)
* [Exploring histograms, an essay](https://tinlizzie.org/histograms/)

### Gode eksempler på visualiseringer, datadreven journalistikk o.l.

* [The hourly heatmap with ggplot2](http://johnmackintosh.com/2016-12-01-the-hourly-heatmap/?utm_content=buffer617db&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer)
* [Overflow Data på Twitter](https://twitter.com/overflow_data)
* [At Chipotle, How Many Calories Do People Really Eat?](https://www.nytimes.com/interactive/2015/02/17/upshot/what-do-people-actually-order-at-chipotle.html?_r=0)
* ["Hvor skal jeg bo"-kart for Helsinki](http://helsinki.wanhala.net/)

### Kart

* [A new kind of map: It's about time](https://blog.mapbox.com/a-new-kind-of-map-its-about-time-7bd9f7916f7f)
* [Isokront togkart over Frankrike](https://i.redd.it/951qo02e8qpy.jpg)

## Kunstig intelligens, generelt

* [AIXI](https://en.wikipedia.org/wiki/AIXI)
* [Solomonoff's theory of inductive inference](https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference)
* [Algorithmic probability](https://en.wikipedia.org/wiki/Algorithmic_probability)

## Statistikk

* [The fallacy of placing confidence in confidence intervals](https://link.springer.com/article/10.3758%2Fs13423-015-0947-8)
* [Brownian bridge](https://en.wikipedia.org/wiki/Brownian_bridge)
* [Hypothesis testing is a bad idea](https://www.reddit.com/r/statistics/comments/5275d6/hypothesis_testing_is_a_bad_idea_whether/)
* [Empirisk Bayes](https://en.wikipedia.org/wiki/Empirical_Bayes_method)
* [Statistiske fallgruver](https://theconversation.com/paradoxes-of-probability-and-other-statistical-strangeness-74440)
* [Inconvergent: On Generative Algorithms](http://inconvergent.net/generative/)

## Diverse matematikk

* [BK-trær](http://signal-to-noise.xyz/post/bk-tree/)
    - Indekserer data over et metrisk rom
* [The First Law of Complexodynamics](http://www.scottaaronson.com/blog/?p=762)
* [Iverson bracket-notasjon for "if"](https://en.wikipedia.org/wiki/Iverson_bracket)